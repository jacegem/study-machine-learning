{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Urban Sound Classification, Part 1\n",
    "\n",
    "출처: \n",
    "- http://aqibsaeed.github.io/2016-09-03-urban-sound-classification-part-1/\n",
    "- https://github.com/aqibsaeed/Urban-Sound-Classification\n",
    "\n",
    "- [Logistic Regression in Tensorflow with SMOTE](http://aqibsaeed.github.io/2016-08-10-logistic-regression-tf/)\n",
    "\n",
    "In this blog post, we will learn techniques to classify urban sounds into categories using machine learning. Earlier blog posts covered classification problems where data can be easily expressed in vector form. For example, in the textual dataset, each word in the corpus becomes feature and tf-idf score becomes its value. Likewise, in anomaly detection dataset we saw two features “throughput” and “latency” that fed into a classifier. But when it comes to sound, feature extraction is not quite straightforward. Today, we will first see what features can be extracted from sound data and how easy it is to extract such features in Python using open source library called Librosa.\n",
    "\n",
    "> 이 블로그 포스트에서는 기계 학습을 사용하여 도시 사운드를 카테고리로 분류하는 기술을 배우게됩니다. 이전 블로그 게시물은 데이터를 벡터 형태로 쉽게 표현할 수있는 분류 문제를 다루었습니다. 예를 들어, 텍스트 데이터 세트에서 코퍼스의 각 단어는 지형지 물이되며 tf-idf 점수는 그 값이됩니다. 마찬가지로, 비정상 탐지 데이터 세트에서 우리는 분류 자에게 공급되는 \"처리량\"과 \"대기 시간\"이라는 두 가지 기능을 보았습니다. 그러나 소리에 관해서, 특징 추출은 아주 간단하지 않습니다. 오늘 우리는 사운드 데이터에서 추출 할 수있는 기능과 Librosa라는 오픈 소스 라이브러리를 사용하여 파이썬에서 이러한 기능을 추출하는 것이 얼마나 쉬운 지 먼저 살펴볼 것입니다.\n",
    "\n",
    "To get started with this tutorial, please make sure you have following tools installed:\n",
    "\n",
    "- Tensorflow\n",
    "- Librosa\n",
    "- Numpy\n",
    "- Matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "We need a labelled dataset that we can feed into machine learning algorithm. Fortunately, some researchers published urban sound dataset. It contains 8,732 labelled sound clips (4 seconds each) from ten classes: air conditioner, car horn, children playing, dog bark, drilling, engine idling, gunshot, jackhammer, siren, and street music. The dataset by default is divided into 10-folds. To get the dataset please visit the following link and if you want to use this dataset in your research kindly don’t forget to acknowledge. In this dataset, the sound files are in .wav format but if you have files in another format such as .mp3, then it’s good to convert them into .wav format. It’s because .mp3 is lossy music compression technique, check this link for more information. To keep things simple, we will use sound files from only first three folds, namely fold1, fold2 and fold3.\n",
    "\n",
    "Let’s read some sound files and visualise to understand how different each sound clip is from other. Matplotlib’s specgram method performs all the required calculation and plotting of the spectrum. Likewise, Librosa provide handy method for wave and log power spectrogram plotting. By looking at the plots shown in Figure 1, 2 and 3, we can see apparent differences between sound clips of different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from matplotlib.pyplot import specgram\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "\n",
    "To extract the useful features from sound data, we will use Librosa library. It provides several methods to extract different features from the sound clips. We are goint to use below mentioned methods to extract various featrues:\n",
    "\n",
    "- melspectrogram: Compute a Mel-scaled power spectrogram\n",
    "- mfcc: Mel-frequency cepstral coefficients\n",
    "- chrorma-stft: Compute a chromagram from a waveform or power spectrogram\n",
    "- spectral_contrast: Compute spectral contrast, using method defined in [Music type classification by spectral contrast feature](http://ieeexplore.ieee.org/document/1035731/)\n",
    "- tonnetz: Computes the tonal centroid features (tonnetz), following the mothod of [Detecting harmonic change in musical audio](https://dl.acm.org/citation.cfm?id=1178727)\n",
    "\n",
    "To make the process of feature extractin from sound clips easy, two helper methods are defined. \n",
    "\n",
    "First `parse_audio_files` which takes parent directory name, subdirectories within parent directory and file extension (default is .wav) as input. It then iterates over all the files within subdirectories and call second helper function `extract_feature`. It takes files path as input, read the file by calling `librosa.load` mehtod, extract and return featrues discssed above.\n",
    "\n",
    "THese two methods are all that is required to convert raw sound clips into informative features (along with a class label for each sound clip) that we can directly feed into ousr classifier.\n",
    "\n",
    "Rememeber, the class label of each sound clip is in the file name. For example, if the file anme is 108041-9-0-4.wav the the class label will be 9. Doing string split by `-` and taking the second item of the array will give us the class label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature(file_name):\n",
    "    X, sample_rate = librosa.load(file_name)\n",
    "    stft = np.abs(librosa.stft(X))\n",
    "    mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T,axis=0)\n",
    "    chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)\n",
    "    mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
    "    contrast = np.mean(librosa.feature.spectral_contrast(S=stft, sr=sample_rate).T,axis=0)\n",
    "    tonnetz = np.mean(librosa.feature.tonnetz(y=librosa.effects.harmonic(X),\n",
    "    sr=sample_rate).T,axis=0)\n",
    "    return mfccs,chroma,mel,contrast,tonnetz\n",
    "\n",
    "def parse_audio_files(parent_dir,sub_dirs,file_ext=\"*.wav\"):\n",
    "    features, labels = np.empty((0,193)), np.empty(0)\n",
    "    for label, sub_dir in enumerate(sub_dirs):\n",
    "        for fn in glob.glob(os.path.join(parent_dir, sub_dir, file_ext)):\n",
    "            try:\n",
    "              mfccs, chroma, mel, contrast,tonnetz = extract_feature(fn)\n",
    "            except Exception as e:\n",
    "              print(\"Error encountered while parsing file: \", fn)\n",
    "              continue\n",
    "            ext_features = np.hstack([mfccs,chroma,mel,contrast,tonnetz])\n",
    "            features = np.vstack([features,ext_features])\n",
    "            labels = np.append(labels, fn.split('/')[2].split('-')[1])\n",
    "    return np.array(features), np.array(labels, dtype = np.int)\n",
    "\n",
    "def one_hot_encode(labels):\n",
    "    n_labels = len(labels)\n",
    "    n_unique_labels = len(np.unique(labels))\n",
    "    one_hot_encode = np.zeros((n_labels,n_unique_labels))\n",
    "    one_hot_encode[np.arange(n_labels), labels] = 1\n",
    "    return one_hot_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir = 'Sound-Data'\n",
    "tr_sub_dirs = [\"fold1\",\"fold2\"]\n",
    "ts_sub_dirs = [\"fold3\"]\n",
    "tr_features, tr_labels = parse_audio_files(parent_dir,tr_sub_dirs)\n",
    "ts_features, ts_labels = parse_audio_files(parent_dir,ts_sub_dirs)\n",
    "\n",
    "tr_labels = one_hot_encode(tr_labels)\n",
    "ts_labels = one_hot_encode(ts_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification using Multilayer Neural Network\n",
    "\n",
    "> Note: If you want to use cikit-learn or any other library for training classifier, feel free to use that. The goal of this tutorial is to provide an implementation of the neural network in Tensorflow for classification tasks.\n",
    "\n",
    "Now we have our training and testing set ready, let's implement two layers neural network in Tensorflow to classify each sound clip into a different category.\n",
    "\n",
    "The code provided below defines configuration parameters required by nerual network model. Such as training epochs, a number of neurones in each hidden layer and learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_epochs = 50\n",
    "n_dim = tr_features.shape[1]\n",
    "n_classes = 10\n",
    "n_hidden_units_one = 280 \n",
    "n_hidden_units_two = 300\n",
    "sd = 1 / np.sqrt(n_dim)\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define placeholders for features and class labels, which tensor flow will fill with the data at runtime. Furthermore, define weights and biases for hidden and output layers of the network. For non-linearity, we use the isgmoid function in the first hidden layer and tanh in the second hidden layer. The output layer has softmax as non-linearity as we are dealing with multiclass classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:170: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(tf.float32,[None,n_dim])\n",
    "Y = tf.placeholder(tf.float32,[None,n_classes])\n",
    "\n",
    "W_1 = tf.Variable(tf.random_normal([n_dim,n_hidden_units_one], mean = 0, stddev=sd))\n",
    "b_1 = tf.Variable(tf.random_normal([n_hidden_units_one], mean = 0, stddev=sd))\n",
    "h_1 = tf.nn.tanh(tf.matmul(X,W_1) + b_1)\n",
    "\n",
    "W_2 = tf.Variable(tf.random_normal([n_hidden_units_one,n_hidden_units_two], \n",
    "mean = 0, stddev=sd))\n",
    "b_2 = tf.Variable(tf.random_normal([n_hidden_units_two], mean = 0, stddev=sd))\n",
    "h_2 = tf.nn.sigmoid(tf.matmul(h_1,W_2) + b_2)\n",
    "\n",
    "W = tf.Variable(tf.random_normal([n_hidden_units_two,n_classes], mean = 0, stddev=sd))\n",
    "b = tf.Variable(tf.random_normal([n_classes], mean = 0, stddev=sd))\n",
    "y_ = tf.nn.softmax(tf.matmul(h_2,W) + b)\n",
    "\n",
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cross-entropy cost funciton will be minimised using gradient descent optimizer, the code provided below initalize cost function and optimizer. Also, define and initalize variables for accuracy calculation of the prediction by model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_function = -tf.reduce_sum(Y * tf.log(y_))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost_function)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We hav eall the requred pieces in place. Now let's train neural network model, visualize whether cost is decreasing with each epoch and make prediction on the test set, using following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (0, 0) for Tensor 'Placeholder_1:0', which has shape '(?, 10)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-a35a61c043f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcost_function\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtr_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtr_labels\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mcost_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost_history\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    973\u001b[0m                 \u001b[0;34m'Cannot feed value of shape %r for Tensor %r, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m                 \u001b[0;34m'which has shape %r'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 975\u001b[0;31m                 % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))\n\u001b[0m\u001b[1;32m    976\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    977\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tensor %s may not be fed.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot feed value of shape (0, 0) for Tensor 'Placeholder_1:0', which has shape '(?, 10)'"
     ]
    }
   ],
   "source": [
    "cost_history = np.empty(shape=[1],dtype=float)\n",
    "y_true, y_pred = None, None\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(training_epochs):            \n",
    "        _,cost = sess.run([optimizer,cost_function],feed_dict={X:tr_features,Y:tr_labels})\n",
    "        cost_history = np.append(cost_history,cost)\n",
    "    \n",
    "    y_pred = sess.run(tf.argmax(y_,1),feed_dict={X: ts_features})\n",
    "    y_true = sess.run(tf.argmax(ts_labels,1))\n",
    "    print(\"Test accuracy: \",round(session.run(accuracy, \n",
    "    \tfeed_dict={X: ts_features,Y: ts_labels}),3))\n",
    "\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "plt.plot(cost_history)\n",
    "plt.axis([0,training_epochs,0,np.max(cost_history)])\n",
    "plt.show()\n",
    "\n",
    "p,r,f,s = precision_recall_fscore_support(y_true, y_pred, average=\"micro\")\n",
    "print(\"F-Score:\", round(f,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we saw how to extract features from a sound dataset and train a two layer neural network model in Tensorflow to categories sounds. I would encourage you to check the documentation of Librosa and experiment with different neural network configurations i.e. by changing number of neurons, number of hidden layers and introducing dropout etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
